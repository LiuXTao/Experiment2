{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据的函数\n",
    "def get_data(file):\n",
    "    data=load_svmlight_file(file)              #调用load_svmlight（）用于读取函数\n",
    "    return data[0].todense(),data[1]            #返回值，第一个为训练特征数据，第二个为训练标签\n",
    "#训练集和测试集读取数据\n",
    "X_train,y_train=get_data('./a9a')\n",
    "X_test,y_test=get_data('./a9a.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取X_train的横纵维度\n",
    "(n,m)=np.shape(X_train)\n",
    "X_train=np.hstack((X_train,np.ones((n,1))))     #为训练数据的每一条数据增加一列,作为bias\n",
    "(n,m)=np.shape(X_test)                           #获取X_test的横纵维度\n",
    "X_test=np.hstack((X_test,np.zeros((n,1))))      #经发现，为第123列没有数据，故补充一列全零列\n",
    "X_test=np.hstack((X_test,np.ones((n,1))))       #为测试数据的每一条数据增加一列,作为bias\n",
    "\n",
    "#将y_train和y_test变为n*1的列向量\n",
    "y_train=np.reshape(y_train,(len(y_train),1))\n",
    "y_test=np.reshape(y_test,(len(y_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义hingeloss损失函数\n",
    "def hingeloss(X,y,w):\n",
    "    l=1-np.multiply(y,X*w)                     #y与X*w先做点乘,相应位置相乘\n",
    "    l2=(l>=0)                                  #然后求出哪个元素大于等于0，化成布尔矩阵l2\n",
    "    result=np.multiply(l,l2)                     #通过l和l2点乘进行筛选，大于等于0的项保留，小于0的项清除变为0,实现hingeloss损失\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hinge loss的公式:\n",
    "$$Hinge loss=\\epsilon_i=max(0-1-y_i(w^Tx_i+b))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准确率计算函数\n",
    "def accuracy(X,w,y):\n",
    "    threshold=0.5                              #设置阈值未0.5\n",
    "    y_predict=X*w                              #得到X，w的预测结果\n",
    "    val=np.multiply(y,y_predict)               #y预测值与y真实值相乘 \n",
    "    N=np.zeros((len(y),1))                  \n",
    "    N[val>=threshold]=1       #y预测值与y真实值相乘,积大于阈值赋值1，表示分类正确,否则为0及分类错误，得到表示结果正确与否的0-1列向量\n",
    "    return np.mean(N)         #得到准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#求梯度函数,其中正则系数C默认值为1\n",
    "def gradient(X,y,w,C=1):\n",
    "    l=1-np.multiply(y,X*w)                       #先做点乘\n",
    "    l2=(l>=0)                                    #然后求出哪个元素大于等于0，化成布尔矩阵\n",
    "    tmp=np.multiply(y,l2)                        #通过点乘进行筛选，大于等于0的项保留，小于0的项清除变为0\n",
    "    return w-C*np.sum(np.multiply(X,tmp),0).T   #计算最终梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度公式：\n",
    "$$gradient=w-\\frac{C}{n}\\sum_{i=1}^{n}(x_iy_i)y\\__i$$\n",
    "其中$y\\__i$=1 if $1-y_i(w^Tx_i)$ else $y\\__i=0$         \n",
    "在上述gradient函数实现中，代码中的C相当于此处C/n,视线中，我将除以n包进C中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#准确率计算函数\n",
    "# def accuracy(X,w,y):\n",
    "#     y_predict=predict(X,w)                  #得到X，w的预测结果\n",
    "#     N=np.zeros((len(y),1))\n",
    "#     N[y_predict==y]=1                       #与y预测值与y真实值进行比较，相同赋值1  否则为0\n",
    "#     return np.sum(N)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#未优化的sgd的实现函数\n",
    "def SGD_gradient(X,y,w,eta,C):\n",
    "    grad=gradient(X,y,w,C)           #计算梯度\n",
    "    w=w-eta*grad                   #进行一步梯度下降\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD实现原理：\n",
    "$$g_t=\\frac{\\partial J(W_{t-1})}{\\partial w}$$\n",
    "$$W_t=W_{t-1}-\\eta g_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAG sgd的实现函数\n",
    "def NAG_gradient(X,y,w,v,gamma,eta,C):\n",
    "    grad=gradient(X,y,w-gamma*v,C)         #计算梯度\n",
    "    v=gamma*v+eta*grad                     #更新v值\n",
    "    w=w-v                                  #梯度下降\n",
    "    return w,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAG实现原理：\n",
    "$$g_t=\\frac{\\partial J}{\\partial w}(W_{t-1}-\\gamma v_{t-1})$$\n",
    "$$v_t=\\gamma v_{t-1}+\\eta g_{t-1}$$\n",
    "$$W_t=W_{t-1}-v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RMS sgd的实现函数\n",
    "def RMS_gradient(X,y,w,G,gamma,eta,epision,C): \n",
    "    grad=gradient(X,y,w,C)                            #计算梯度\n",
    "    G=gamma*G+(1-gamma)*np.square(grad)                    #G值由过去的G值和新的梯度点乘求得\n",
    "    dot=np.multiply(eta/(np.sqrt(G+epision)),grad)             \n",
    "    w=w-dot                                                #梯度下降\n",
    "    return w,G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS实现原理：\n",
    "$$g_t=\\frac{\\partial J(W_{t-1})}{\\partial w}$$\n",
    "$$G_t=\\gamma G_t+(1-\\gamma)g_t·g_t)$$\n",
    "\n",
    "$$W_t=W_{t-1}-\\frac{\\eta}{\\sqrt{G_t+\\epsilon }}·g_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adadelta sgd实现\n",
    "def Adadelta_gradient(X,y,w,G,gamma,delta,epision,C):\n",
    "    grad=gradient(X,y,w,C)                       #计算梯度\n",
    "    dot=np.square(grad)                            #梯度进行点乘\n",
    "    G=gamma*G+(1-gamma)*dot                     #G值由过去的G值和新的梯度点乘求得\n",
    "    delta_w=np.multiply((np.sqrt(delta+epision)/np.sqrt(G+epision)),grad) #w_delta计算\n",
    "    w=w-delta_w                                      #梯度下降\n",
    "    delta=gamma*delta+(1-gamma)*(np.square(delta_w))  #新delta由过去的delta值和新的w_delta的点乘求得\n",
    "    return w,G,delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adadelta实现原理\n",
    "$$g_t=\\Delta J(W_{t-1}$$\n",
    "$$G_t=\\gamma W_t+(1-\\gamma)g_t·g_t$$\n",
    "$$\\Delta W_t=-\\frac{\\sqrt{\\Delta _{t-1}+\\epsilon}}{\\sqrt{G_t+\\epsilon }}·g_t$$\n",
    "$$W_t=W_{t-1}+\\Delta W_t$$\n",
    "$$\\Delta _t=\\gamma \\Delta _{t-1}+(1-\\gamma)\\Delta W_t·\\Delta W_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adam sgd实现\n",
    "def Adam_gradient(X,y,w,G,m,gamma,t,eta,belta,epision,C):\n",
    "    grad=gradient(X,y,w,C)                                     #计算梯度\n",
    "    m=belta*m+(1-belta)*grad                                         #根据原有的m值和新求得grad计算m\n",
    "    G=gamma*G+(1-gamma)*(np.multiply(grad,grad))                            #G值由过去的G值和新的梯度点乘求得\n",
    "    alpha=eta*(np.sqrt(1-math.pow(gamma,t))/(1-math.pow(belta,t)))  #计算alpha\n",
    "    w=w-alpha*(m/(np.sqrt(G+epision)))                             #梯度下降\n",
    "    return w,G,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam的实现原理：\n",
    "$$g_t=\\frac{\\partial J(W_{t-1})}{\\partial} w$$\n",
    "$$m_t=\\beta _1m_{t-1}+(1-\\beta)g_t$$\n",
    "$$G_t=\\gamma G_t+(1-\\gamma)g_t·g_t$$\n",
    "$$\\alpha=\\eta\\frac{\\sqrt{1-\\gamma ^t}}{1-\\beta ^t}$$\n",
    "$$\\Theta=\\Theta_{t-1}-\\alpha\\frac{m_t}{\\sqrt{G_t+\\epsilon}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分别为NAG,RMSProb,Adadelta,Adam初始化权重向量等\n",
    "(n,m)=np.shape(X_train)\n",
    "#SGD的初始化权重矩阵\n",
    "w_sgd=np.zeros((m,1))\n",
    "\n",
    "#NAG的初始化权重矩阵\n",
    "w_nag=np.zeros((m,1))\n",
    "v_nag=np.zeros((m,1))\n",
    "#RMS的初始化权重矩阵\n",
    "w_rms=np.zeros((m,1))\n",
    "#w_rms=np.random.standard_normal([X_train.shape[1],1])\n",
    "G_rms=np.zeros((m,1))\n",
    "#Adadelta的初始化权重矩阵\n",
    "w_ada=np.zeros((m,1))\n",
    "G_ada=np.zeros((m,1))\n",
    "delta=np.zeros((m,1))\n",
    "#Adam的初始化权重矩阵\n",
    "w_adam=np.zeros((m,1))\n",
    "G_adam=np.zeros((m,1))\n",
    "m_adam=np.zeros((m,1))\n",
    "\n",
    "#迭次次数\n",
    "epco=4000\n",
    "times=range(epco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义未优化的SGD函数\n",
    "def SGD(X,y,w_sgd,eta,train_size,epco,C,gradient=SGD_gradient,loss=hingeloss,accuracy=accuracy):\n",
    "    sgd_train=[]                       #训练误差列表\n",
    "    sgd_test=[]                            #测试误差列表\n",
    "    sgd_accuracy_train=[]                   #训练准确率列表\n",
    "    sgd_accuracy_test=[]                    #测试准确率列表\n",
    "    #开始进行梯度下降过程\n",
    "    for i in range(epco):\n",
    "        #得到用于梯度下降的小部分样本，即X_trainset,y_trainset\n",
    "        X_trainset,X_other,y_trainset,y_other=train_test_split(X,y,test_size=1-train_size,random_state=random.randint(0,1000))\n",
    "        #调用未优化的sgd的梯度下降函数，每一次迭代进行一步梯度下降\n",
    "        w_sgd=gradient(X_trainset,y_trainset,w_sgd,eta,C)\n",
    "        sgd_train.append(loss(X_trainset,y_trainset,w_sgd))            #得到训练和测试的loss\n",
    "        sgd_test.append(loss(X_test,y_test,w_sgd))\n",
    "        sgd_accuracy_train.append(accuracy(X_train,w_sgd,y_train))     #计算训练和测试的准确率\n",
    "        sgd_accuracy_test.append(accuracy(X_test,w_sgd,y_test))\n",
    "    return sgd_train,sgd_test,sgd_accuracy_train,sgd_accuracy_test\n",
    "#参数初始化\n",
    "eta=0.00005                           #学习率设为0.008\n",
    "train_size=0.0005               #指用于训练SGD的样本数占X_train的比例,约有15个样本用于训练\n",
    "C=1\n",
    "#开始训练，得到训练误差，测试误差，训练准确率和测试准确率\n",
    "sgd_train,sgd_test,sgd_accuracy_train,sgd_accuracy_test=SGD(X=X_train,y=y_train,w_sgd=w_sgd,eta=eta,train_size=train_size,epco=epco,C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义NAG sgd的训练函数\n",
    "def NAG(X,y,w_nag,v_nag,gamma,eta,train_size,C,gradient=NAG_gradient,loss=hingeloss,accuracy=accuracy):\n",
    "    nag_train=[]                     #训练误差列表\n",
    "    nag_test=[]                      #测试误差列表\n",
    "    nag_accuracy_train=[]            #训练准确率列表\n",
    "    nag_accuracy_test=[]             #测试准确率列表\n",
    "    for i in range(epco):\n",
    "        #得到用于随机梯度下降训练的小部分样本  {即X_trainset,y_trainset}\n",
    "        X_trainset,X_other,y_trainset,y_other=train_test_split(X,y,test_size=1-train_size,random_state=random.randint(0,1000))\n",
    "        #调用NG_gradient,每一次迭代进行一次梯度下降\n",
    "        w_nag,v_nag=gradient(X_trainset,y_trainset,w_nag,v_nag,gamma,eta,C)\n",
    "        nag_train.append(loss(X_trainset,y_trainset,w_nag))                #得到训练和测试的loss\n",
    "        nag_test.append(loss(X_test,y_test,w_nag))\n",
    "        nag_accuracy_train.append(accuracy(X_trainset,w_nag,y_trainset))  #计算训练和测试的准确率\n",
    "        nag_accuracy_test.append(accuracy(X_test,w_nag,y_test))\n",
    "    return nag_train,nag_test,nag_accuracy_train,nag_accuracy_test\n",
    "#参数初始化\n",
    "gamma=0.9\n",
    "eta=0.00001                                                             #学习率设为0.001\n",
    "train_size=0.0005                                                       #指用于训练SGD的样本数占X_train的比例\n",
    "C=1\n",
    "#开始训练，得到训练误差，测试误差，训练准确率和测试准确率\n",
    "nag_train,nag_test,nag_accuracy_train,nag_accuracy_test=NAG(X=X_train,y=y_train,w_nag=w_nag,v_nag=v_nag,gamma=gamma,eta=eta,train_size=train_size,C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义RMS sgd的实现方法\n",
    "def RMS(X,y,w_rms,G_rms,gamma,eta,epco,train_size,C,gradient=RMS_gradient,loss=hingeloss,accuracy=accuracy):\n",
    "    epision=0.00001                   #设置epision为0.0001，既避免Gt为零时的错误，也减小该参数对结果的影响   \n",
    "    rms_train=[]                     #训练误差列表\n",
    "    rms_test=[]                      #测试误差列表\n",
    "    rms_accuracy_train=[]           #训练准确率列表\n",
    "    rms_accuracy_test=[]            #测试准确率列表\n",
    "    for i in range(epco):\n",
    "        #得到用于随机梯度下降训练的小部分样本\n",
    "        X_trainset,X_other,y_trainset,y_other=train_test_split(X,y,test_size=1-train_size,random_state=random.randint(0,1000))\n",
    "        w_rms,G_rms=gradient(X_trainset,y_trainset,w_rms,G_rms,gamma,eta,epision,C)    #梯度下降更新\n",
    "        rms_train.append(loss(X_trainset,y_trainset,w_rms))              #得到训练和测试的loss\n",
    "        rms_test.append(loss(X_test,y_test,w_rms))\n",
    "        rms_accuracy_train.append(accuracy(X_trainset,w_rms,y_trainset))  #计算准确率\n",
    "        rms_accuracy_test.append(accuracy(X_test,w_rms,y_test))\n",
    "    return rms_train,rms_test,rms_accuracy_train,rms_accuracy_test\n",
    "#参数初始化\n",
    "gamma=0.9\n",
    "eta=0.001                                                        #学习率设为0.001\n",
    "train_size=0.0005                                                #指用于训练SGD的样本数占X_train的比例\n",
    "C=1\n",
    "#开始训练，得到训练误差，测试误差，训练准确率和测试准确率\n",
    "rms_train,rms_test,rms_accuracy_train,rms_accuracy_test=RMS(X=X_train,y=y_train,w_rms=w_rms,G_rms=G_rms,gamma=gamma,eta=eta,epco=epco,train_size=train_size,C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Adadelta sgd的实现方法\n",
    "def Adadelta(X,y,w_ada,G_ada,delta,gamma,eta,epco,train_size,C,gradient=Adadelta_gradient,loss=hingeloss,accuracy=accuracy):\n",
    "    epision=0.0000001                           #设置epision为0.0001，既避免Gt为零时的错误，也减小该参数对结果的影响               \n",
    "    ada_train=[]                               #训练误差列表\n",
    "    ada_test=[]                                   #测试误差列表\n",
    "    ada_accuracy_train=[]                           #训练准确率列表\n",
    "    ada_accuracy_test=[]                               #测试准确率列表\n",
    "    for i in range(epco):\n",
    "        #得到用于随机梯度下降训练的小部分样本\n",
    "        X_trainset,X_other,y_trainset,y_other=train_test_split(X,y,test_size=1-train_size,random_state=random.randint(0,1000))        \n",
    "        #调用Adadelta_gradient,每一次迭代进行一次梯度下降\n",
    "        w_ada,G_ada,delta=gradient(X_trainset,y_trainset,w_ada,G_ada,gamma,delta,epision,C)\n",
    "        ada_train.append(loss(X_trainset,y_trainset,w_ada))           #得到训练和测试的loss\n",
    "        ada_test.append(loss(X_test,y_test,w_ada))\n",
    "        ada_accuracy_train.append(accuracy(X_trainset,w_ada,y_trainset)) #计算训练和测试准确率\n",
    "        ada_accuracy_test.append(accuracy(X_test,w_ada,y_test))\n",
    "    \n",
    "    return ada_train,ada_test,ada_accuracy_train,ada_accuracy_test\n",
    "#参数初始化\n",
    "gamma=0.9\n",
    "eta=0.000001                                               #学习率设为0.001\n",
    "train_size=0.0005                                        #指用于训练SGD的样本数占X_train的比例\n",
    "C=1\n",
    "#开始训练，得到训练误差，测试误差，训练准确率和测试准确率\n",
    "ada_train,ada_test,ada_accuracy_train,ada_accuracy_test=Adadelta(X=X_train,y=y_train,w_ada=w_ada,G_ada=G_ada,delta=delta,gamma=gamma,eta=eta,epco=epco,train_size=train_size,C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Adam sgd的实现方法\n",
    "def Adam(X,y,w_adam,G_adam,m_adam,gamma,eta,epco,train_size,C,gradient=Adam_gradient,loss=hingeloss,accuracy=accuracy):\n",
    "    epision=0.0001                          #设置epision为0.0001，既避免Gt为零时的错误，也减小该参数对结果的影响\n",
    "    adam_train=[]                       #训练误差列表\n",
    "    adam_test=[]                         #测试误差列表\n",
    "    adam_accuracy_train=[]                #训练准确率列表\n",
    "    adam_accuracy_test=[]                 #测试准确率列表\n",
    "    for i in range(epco):\n",
    "        #得到用于随机梯度下降训练的小部分样本\n",
    "        X_trainset,X_other,y_trainset,y_other=train_test_split(X,y,test_size=1-train_size,random_state=random.randint(0,1000))        \n",
    "        # 调用Adam_gradient,每一次迭代进行一次梯度下降\n",
    "        w_adam,G_adam,m_adam=gradient(X_trainset,y_trainset,w_adam,G_adam,m_adam,gamma,i+1,etha,belta,epision,C)\n",
    "        adam_train.append(loss(X_trainset,y_trainset,w_adam))              #得到训练和测试的loss\n",
    "        adam_test.append(loss(X_test,y_test,w_adam))\n",
    "        adam_accuracy_train.append(accuracy(X_trainset,w_adam,y_trainset))  #计算训练和测试准确率\n",
    "        adam_accuracy_test.append(accuracy(X_test,w_adam,y_test))\n",
    "    return adam_train,adam_test,adam_accuracy_train,adam_accuracy_test\n",
    "#参数初始化\n",
    "gamma=0.9\n",
    "etha=0.001                                       #学习率设为0.01\n",
    "belta=0.9                                      \n",
    "train_size=0.0005                                 #指用于训练SGD的样本数占X_train的比例\n",
    "C=1\n",
    "#开始训练，得到训练误差，测试误差，训练准确率和测试准确率\n",
    "adam_train,adam_test,adam_accuracy_train,adam_accuracy_test=Adam(X=X_train,y=y_train,w_adam=w_adam,G_adam=G_adam,m_adam=m_adam,gamma=gamma,eta=eta,epco=epco,train_size=train_size,C=C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd accuracy 0.757508752534\n",
      "nag accuracy: 0.741047847184\n",
      "rmsprob accuracy: 0.72120877096\n",
      "adadelta accuracy: 0.718936183281\n",
      "adam accuracy: 0.718260549106\n"
     ]
    }
   ],
   "source": [
    "#打印出每种方法最后的准确率\n",
    "print('sgd accuracy',sgd_accuracy_test[-1])\n",
    "print('nag accuracy:',nag_accuracy_test[-1])\n",
    "print(\"rmsprob accuracy:\",rms_accuracy_test[-1])\n",
    "print(\"adadelta accuracy:\",ada_accuracy_test[-1])\n",
    "print(\"adam accuracy:\",adam_accuracy_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3293c3630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#打印出各种实现方法的loss曲线\n",
    "plt.yscale('log')\n",
    "plt.title(\"SVM Linear Classification with SGD(loss)\")\n",
    "plt.xlabel('epcohs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(times,sgd_test,label=\"SGD\")\n",
    "plt.plot(times,nag_test,label=\"NAG\")\n",
    "plt.plot(times,ada_test,label=\"AdaDelta\")\n",
    "plt.plot(times,rms_test,label=\"RMSprop\")\n",
    "plt.plot(times,adam_test,label=\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #打印出各种是先放的准确率曲线\n",
    "# plt.title('SVM Linear Classification with SGD(accuracy)')\n",
    "# plt.xlabel('times')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.plot(times,nag_accuracy_test,label=\"NAG\")\n",
    "# plt.plot(times,rms_accuracy_test,label=\"RMSprop\")\n",
    "# plt.plot(times,ada_accuracy_test,label=\"AdaDelta\")\n",
    "# plt.plot(times,adam_accuracy_test,label=\"Adam\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
